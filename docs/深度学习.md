# Deep Learning

## Why Deep Learning?

Deep Learning与Machine Learning的一个区别是：Machine Learning需要选择特征

怎么提取特征 → 怎么设计结构

隐藏层就是在做特征工程

既然一个隐藏层就可以拟合所有函数，为什么还需要多个隐藏测？(Why deep? not fat?)

一个惊人的事实是：达到同样的效果，Deep Learning需要的参数反而更少 (可以理解为加layer是乘法，加神经元是加法)

## Neural Network

### 激活函数

为什么要使用激活函数？引入非线性

对于分类问题，最后一个全连接层不要使用激活函数（实际上nn.CrossEntropyLoss()计算loss时包含了softmax，实际上osftmax就是激活函数，但是不需要自行添加）

## Convolutional Neural Network (CNN)

1. 局部连接：  
    CNN通过卷积层实现局部连接。每个卷积层的卷积核（滤波器）只在输入图像的一小部分上进行操作，提取局部特征。这样，CNN能够捕捉到图像中的局部模式，如边缘、角点等。
2. 权重共享：  
    卷积核在整个输入图像上滑动，生成特征图。由于同一卷积核在不同位置共享权重，CNN的参数数量大大减少，从而提高了模型的计算效率和泛化能力。
3. 平移不变性：
    卷积操作使得CNN对输入图像的平移具有一定的鲁棒性。这意味着即使图像中的某个特征位置发生变化，CNN仍然可以识别该特征。
4. 层次化特征表示：
    CNN通过多个卷积层和池化层逐步提取图像的层次化特征。较低层次的卷积层捕捉到简单的局部特征（如边缘），而较高层次的卷积层则能够提取更复杂的特征（如对象的一部分）。

### Convolution

卷积可以观察到一些小尺度的特征，堆叠多层卷积核有更大的感受域，能感受到更大的范围

卷积核的参数和偏置也是可以训练的

卷积核大小（长宽相等）（kernel_size）  
填充（上下左右都增加相等）(padding)  
步幅（水平垂直移动相等）(stride)

$$
卷积后长/宽 = (原长/宽 - kerne\_ size + 2 * padding) / stride + 1
$$

(如果padding不对称就是左右padding和)

$$
上一层感受野长宽 = (下一层感受野长宽 - 1) \times stride + kerne\_ size
$$

卷积核的数量 = 输出通道数量  
卷积核的维度 = K x K x 输入通道数量  
卷积核参数数量 = 输出通道数 x ( K x K x 输入通道数 + 1)  
每个卷积核在不同输入通道的权重不相同但偏置相同

### Transposed Convolution

转置卷积，上采样（使特征图长宽增加）

### Dilated Convolution（Atrous Convolution）

膨胀卷积（空洞卷积），增大感受野，r要服从HDC设计准则，需要覆盖不能有空隙

### Pooling

池化可以减少参数，但是会丢失一些有效信息，非必要

## ResNet

ResNet的引入解决了深层神经网络训练中存在的“退化”问题，即随着网络层数的增加，训练误差反而变得更高。

原理：由于在没有shortcut的网络中，深层网络的输出可能无法输出浅层的输入（即不存在恒等映射 $f(x)=x$），使得模型的值域发生偏移，可能使结果变差。而直接加入 $x$ ，使得输出包含恒等映射（即使残差块内所有参数为0，输出也为 $x$），使得在理论上模型至少不会变差

## DenseNet

## Recurrent Neural Network (RNN)

### Long Short-Term Memory (LSTM)

1. input gate
2. forget gate
3. output gate

## Graph Neural Network (GNN)

## Attention 注意力机制

1. 动态加权：

    注意力机制通过动态地分配权重来选择重要的信息。它根据输入数据的不同部分的重要性，分配不同的注意力权重，从而聚焦于最相关的部分。

2. 计算注意力权重：

    注意力机制通常使用查询（Query）、键（Key）和值（Value）来计算注意力权重。具体步骤如下：

    1. 计算查询和键之间的相似度（如点积）。
    2. 使用Softmax函数将相似度转化为注意力权重。
    3. 使用注意力权重对值进行加权求和，得到加权后的输出。

3. 解决长程依赖问题：

    注意力机制能够有效地捕捉序列数据中的长程依赖关系。相比于传统的RNN和LSTM，注意力机制可以直接在全局范围内分配注意力，而不受序列长度的限制。

4. 应用于Transformer：

    注意力机制是Transformer模型的核心组件，特别是自注意力机制（Self-Attention）。在Transformer中，每个输入的位置都可以与其他所有位置进行交互，从而实现全局信息的整合。这使得Transformer在处理自然语言处理任务（如机器翻译、文本生成）方面表现出色。

矩阵 k q v

### Multi-head attention

## Transformer

1. 输出的token是一个整数
2. 由嵌入层转换成多维的嵌入向量
3. 经encoder和decoder对嵌入向量处理
4. 全连接维度变为词汇表大小并输出，表示输出每个token的概率
5. decoder是递归产生每个token的，decoder已经输出的所有token序列都会再输入encoder以生成下一个token

### 位置编码

为什么需要位置编码？位置也有信息  
这里举一个例子：ass raby hoky irutuion 和 ass irutuion raby hoky，这里 ass 由注意力机制计算得到的向量是一样的，并没有体现出其他符号位置变动导致的差异

### Encoder

encoder计算self-attention

基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机

### Decoder

Decoder 生成目标序列是以一个自回归（auto-regressive）的过程，需要多次迭代生成完整的输出。由已产生的 token 序列进行 masked self-attention 生成 q，与 encoder 输出的 k v 进行交叉注意力

输入是 Encoder 和已经产生的输出（训练时是正确的结果，测试时则是上一步的输出，如果有错误产生，可能导致连锁反应，即接连出错，考虑在训练数据中添加一些错误情况）

输出是 token 长度和输入的已生成的长度相同，但是推理时只关注选择最后一个 token 作为新生成的 token

解释 decoder 的 auto-regressive（start 为起始 token，end 为终结 token）: 

1. 输入 [start ]，输出 ['I' ]，选择 'I' 作为新的 token
2. 输入 [start, 'I']，输出 [xxx, 'love']，选择 'love' 作为新的 token
3. 输入 [start, 'I', 'love']，输出 [xxx, xxx, 'you']，选择 'you' 作为新的 token
4. 输入 [start, 'I', 'love', 'you']，输出 [xxx, xxx, xxx, end]，结束

## Concepts

### 预训练（pre-learning）、微调（fine-tuning）、迁移学习（transfer learning）

### Encoder-Decoder

### Batch Normalization
 
对每个特征做 Normalization

### Layer Normalization

对每个样本做 Normalization

其他小概念：

- End2End：即端到端，从原始输入到最终输出的整个过程由一个单一的模型或系统直接完成
- Seq2Seq：即序列到序列模型，针对输入输出序列长度不确定的问题
- Ground Truth：其实就是真实答案
- 正/负样本：正样本是指包含目标对象的样本，负样本是指不包含目标对象的样本

## Problems

### 梯度下降 Loss 局部最优解

### Underfitting

### Overfitting

1. 使用验证集 个人认为，测试集数据只应作为最后结果评估，只能在测试集上测试一次，不应该使用测试集调整模型，实际这部分应该由验证集完成
2. Dropout（分类问题使用Dropout，回归问题不要使用）
3. 减少模型参数
4. 增加训练数据

### 梯度消失、梯度爆炸

批量归一化（Batch Normalization，简称BN）是一种加速神经网络训练、稳定训练过程并提高网络性能的技术。批量归一化通过标准化每一小批数据（batch）的输入，使得每一层的输入保持在一个相对稳定的分布，从而减轻了内部协变量偏移（internal covariate shift）的问题。

> BN注意：
>
> 1. 训练时model.train(), 评估时设置 model.eval()
> 2. batch size越大越好
> 3. 先卷积，再bn，最后激活，且卷积不需要偏置也没用

### 权重初始化

<https://blog.csdn.net/weixin_39653948/article/details/107950764>

## Trick

1. 分类：[dummy class](https://www.zhihu.com/question/347847220/answer/2895963746)

## TODO

1. GAN
2. Self-supervised Learning
3. BERT
4. GPT
