# Deep Learning

## Why Deep Learning?

Deep Learning与Machine Learning的一个区别是：Machine Learning需要选择特征

怎么提取特征 → 怎么设计结构

隐藏层就是在做特征工程

既然一个隐藏层就可以拟合所有函数，为什么还需要多个隐藏测？(Why deep? not fat?)

一个惊人的事实是：达到同样的效果，Deep Learning需要的参数反而更少 (可以理解为加layer是乘法，加神经元是加法)

## Neural Network

### 激活函数

为什么要使用激活函数？引入非线性

对于分类问题，最后一个全连接层不要使用激活函数（实际上nn.CrossEntropyLoss()计算loss时包含了softmax，实际上osftmax就是激活函数，但是不需要自行添加）

## Convolutional Neural Network (CNN)

1. 局部连接：  

    CNN通过卷积层实现局部连接。每个卷积层的卷积核（滤波器）只在输入图像的一小部分上进行操作，提取局部特征。这样，CNN能够捕捉到图像中的局部模式，如边缘、角点等。

2. 权重共享：  

    卷积核在整个输入图像上滑动，生成特征图。由于同一卷积核在不同位置共享权重，CNN的参数数量大大减少，从而提高了模型的计算效率和泛化能力。

3. 平移不变性：

    卷积操作使得CNN对输入图像的平移具有一定的鲁棒性。这意味着即使图像中的某个特征位置发生变化，CNN仍然可以识别该特征。

4. 层次化特征表示：

    CNN通过多个卷积层和池化层逐步提取图像的层次化特征。较低层次的卷积层捕捉到简单的局部特征（如边缘），而较高层次的卷积层则能够提取更复杂的特征（如对象的一部分）。

### Convolution

卷积可以观察到一些小尺度的特征，堆叠多层卷积核有更大的感受域，能感受到更大的范围

卷积核的参数和偏置也是可以训练的

卷积核大小（长宽相等）（kernel_size）  
填充（上下左右都增加相等）(padding)  
步幅（水平垂直移动相等）(stride)

$$
卷积后长/宽 = (原长/宽 - kerne\_ size + 2 * padding) / stride + 1
$$

(如果padding不对称就是左右padding和)

$$
上一层感受野长宽 = (下一层感受野长宽 - 1) \times stride + kerne\_ size
$$

卷积核的数量 = 输出通道数量  
卷积核的维度 = K x K x 输入通道数量  
卷积核参数数量 = 输出通道数 x ( K x K x 输入通道数 + 1)  
每个卷积核在不同输入通道的权重不相同但偏置相同

### Pooling

池化可以减少参数，但是会丢失一些有效信息，非必要

## ResNet

ResNet的引入解决了深层神经网络训练中存在的“退化”问题，即随着网络层数的增加，训练误差反而变得更高。

原理：由于在没有shortcut的网络中，深层网络的输出可能无法输出浅层的输入（即不存在恒等映射 $f(x)=x$），使得模型的值域发生偏移，可能使结果变差。而直接加入 $x$ ，使得输出包含恒等映射（即使残差块内所有参数为0，输出也为 $x$），使得在理论上模型至少不会变差

## DenseNet

## Recurrent Neural Network (RNN)

### Long Short-Term Memory (LSTM)

1. input gate
2. forget gate
3. output gate

## Graph Neural Network (GNN)

## Attention 注意力机制

1. 动态加权：

    注意力机制通过动态地分配权重来选择重要的信息。它根据输入数据的不同部分的重要性，分配不同的注意力权重，从而聚焦于最相关的部分。

2. 计算注意力权重：

    注意力机制通常使用查询（Query）、键（Key）和值（Value）来计算注意力权重。具体步骤如下：

    1. 计算查询和键之间的相似度（如点积）。
    2. 使用Softmax函数将相似度转化为注意力权重。
    3. 使用注意力权重对值进行加权求和，得到加权后的输出。

3. 解决长程依赖问题：

    注意力机制能够有效地捕捉序列数据中的长程依赖关系。相比于传统的RNN和LSTM，注意力机制可以直接在全局范围内分配注意力，而不受序列长度的限制。

4. 应用于Transformer：

    注意力机制是Transformer模型的核心组件，特别是自注意力机制（Self-Attention）。在Transformer中，每个输入的位置都可以与其他所有位置进行交互，从而实现全局信息的整合。这使得Transformer在处理自然语言处理任务（如机器翻译、文本生成）方面表现出色。

矩阵 k q v

### Multi-head attention

## Transformer

1. 输出的token是一个整数
2. 由嵌入层转换成多维的嵌入向量
3. 经encoder和decoder对嵌入向量处理
4. 全连接维度变为词汇表大小并输出，表示输出每个token的概率

### 位置编码

为什么需要位置编码？位置也有信息  
这里举一个例子：ass raby hoky irutuion 和 ass irutuion raby hoky，这里 ass 由注意力机制计算得到的向量是一样的，并没有体现出其他符号位置变动导致的差异

### Encoder

encoder计算self-attention

基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机

### Decoder

已输出的self-attention和上一个输出的token和encoder的attention

输入是Encoder和已经产生的输出 (训练时是正确的结果，测试时则是上一步的输出，如果有错误产生，可能导致连锁反应，即接连出错，考虑在训练数据中添加一些错误情况)

## 迁移学习

## Concepts

### Seq2Seq

### End2End

## Problems

### 梯度下降 Loss 局部最优解

### Underfitting

### Overfitting

1. 使用验证集 个人认为，测试集数据只应作为最后结果评估，只能在测试集上测试一次，不应该使用测试集调整模型，实际这部分应该由验证集完成
2. Dropout
3. 减少模型参数
4. 增加训练数据

### 梯度消失、梯度爆炸

批量归一化（Batch Normalization，简称BN）是一种加速神经网络训练、稳定训练过程并提高网络性能的技术。批量归一化通过标准化每一小批数据（batch）的输入，使得每一层的输入保持在一个相对稳定的分布，从而减轻了内部协变量偏移（internal covariate shift）的问题。

> BN注意：
>
> 1. 训练时model.train(), 评估时设置 model.eval()
> 2. batch size越大越好
> 3. 先卷积，再bn，最后激活，且卷积不需要偏置也没用

### 权重初始化

<https://blog.csdn.net/weixin_39653948/article/details/107950764>

## Trick

1. 分类：[dummy class](https://www.zhihu.com/question/347847220/answer/2895963746)

## TODO

1. GAN
2. Self-supervised Learning
3. BERT
4. GPT
