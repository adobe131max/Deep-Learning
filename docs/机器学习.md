# Machine Learning

Steps:

1. function(model)
2. define Loss function
3. optimization(gredient descent)

损失函数 $L$  
所有参数记为：$\theta$，初始随机参数为：$\theta^0$  
学习率：$\eta$  
每次update计算的梯度 $g = \nabla L(\theta^i)$  
$$
\theta^{i+1}=\theta^i-\eta g
$$

update: 把训练数据分成多个batch，每次使用一个batch更新参数  
epoch: 一个epoch是把所有batch都update一次，也就是所有训练数据都过一遍

为什么要分batch?TODO

前向传播：计算结果

反向传播：调整参数

全连接

## 多层感知器（Multilayer Perceptron, MLP）

线性模型存在很多局限性，使用非线性的激活函数
